# MNIST CNN Model Training - Version 1

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A deep learning project implementing a Convolutional Neural Network (CNN) for MNIST digit classification using PyTorch. This is the first iteration of our model development journey.

## ğŸ“ Project Structure

```
tsai_assignment_5/
â”œâ”€â”€ mnist_model_training_v1.ipynb    # Main training notebook
â”œâ”€â”€ README.md                        # Project documentation
â””â”€â”€ data/
    â””â”€â”€ MNIST/
        â””â”€â”€ raw/                     # MNIST dataset files
```

## ğŸ¯ Objective

Develop a CNN model that can achieve high accuracy on MNIST digit classification while maintaining reasonable computational efficiency.

## ğŸ—ï¸ Model Architecture

Our CNN model (`CNN_Model`) follows a deep convolutional architecture with the following layers:

### Network Structure
```
Input Layer (1, 28, 28)
â”œâ”€â”€ Conv2d(1â†’8, kernel=3, padding=1) + BatchNorm2d + ReLU
â”œâ”€â”€ Conv2d(8â†’16, kernel=3, padding=1) + BatchNorm2d + ReLU + MaxPool2d(2,2)
â”œâ”€â”€ Conv2d(16â†’32, kernel=3, padding=1) + BatchNorm2d + ReLU
â”œâ”€â”€ Conv2d(32â†’64, kernel=3, padding=1) + BatchNorm2d + ReLU + MaxPool2d(2,2)
â”œâ”€â”€ Conv2d(64â†’128, kernel=3, padding=1) + BatchNorm2d + ReLU
â”œâ”€â”€ Conv2d(128â†’256, kernel=3, padding=1) + AdaptiveAvgPool2d(1)
â”œâ”€â”€ Linear(256â†’100) + ReLU
â””â”€â”€ Linear(100â†’10)
```

### Key Features
- **6 Convolutional Layers**: Progressive channel expansion (1â†’8â†’16â†’32â†’64â†’128â†’256)
- **Batch Normalization**: Applied after each convolution for training stability
- **ReLU Activation**: Standard activation function for non-linearity
- **Adaptive Global Average Pooling**: Reduces spatial dimensions efficiently
- **2 Fully Connected Layers**: Final classification layers

## ğŸ“Š Model Parameters

```python
================================================================
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 28, 28]              80
       BatchNorm2d-2            [-1, 8, 28, 28]              16
            Conv2d-3           [-1, 16, 28, 28]           1,168
       BatchNorm2d-4           [-1, 16, 28, 28]              32
            Conv2d-5           [-1, 32, 14, 14]           4,640
       BatchNorm2d-6           [-1, 32, 14, 14]              64
            Conv2d-7           [-1, 64, 14, 14]          18,496
       BatchNorm2d-8           [-1, 64, 14, 14]             128
            Conv2d-9          [-1, 128, 7, 7]            73,856
      BatchNorm2d-10          [-1, 128, 7, 7]             256
           Conv2d-11          [-1, 256, 7, 7]           295,168
           Linear-12                  [-1, 100]            25,700
           Linear-13                   [-1, 10]             1,010
================================================================
Total params: 420,614
Trainable params: 420,614
Non-trainable params: 0
================================================================
```

**Total Parameters**: 420,614

## ğŸ”§ Training Configuration

### Hyperparameters
- **Optimizer**: Adam
- **Learning Rate**: 0.001
- **Batch Size**: 128 (training), 2000 (testing)
- **Epochs**: 20
- **Loss Function**: CrossEntropyLoss
- **Device**: CUDA (if available) / CPU

### Data Preprocessing
- **Normalization**: Mean = 0.1307, Std = 0.3081 (calculated from training data)
- **Transform Pipeline**: ToTensor() â†’ Normalize()

## ğŸ“ˆ Training Results

### Final Performance Metrics
| Metric | Training | Testing |
|--------|----------|---------|
| **Final Accuracy** | XX.XX% | XX.XX% |
| **Final Loss** | X.XXXX | X.XXXX |

### Training Evolution
The model was trained for 20 epochs with the following progression:

```
Epoch [1/20]  - Train Loss: X.XXXX, Train Acc: XX.XX% - Test Loss: X.XXXX, Test Acc: XX.XX%
Epoch [2/20]  - Train Loss: X.XXXX, Train Acc: XX.XX% - Test Loss: X.XXXX, Test Acc: XX.XX%
...
Epoch [20/20] - Train Loss: X.XXXX, Train Acc: XX.XX% - Test Loss: X.XXXX, Test Acc: XX.XX%
```

### Performance Visualization
The notebook includes comprehensive visualization of:
- **Training vs Test Loss** curves over 20 epochs
- **Training vs Test Accuracy** curves over 20 epochs
- **Sample predictions** on test data

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install torch torchvision matplotlib torchsummary
```

### Running the Model
1. Clone this repository
2. Open `mnist_model_training_v1.ipynb` in Jupyter Notebook or VS Code
3. Run all cells sequentially
4. View training progress and final results

### Key Notebook Sections
1. **Data Loading & Preprocessing**: MNIST dataset preparation
2. **Model Definition**: CNN architecture implementation
3. **Training Loop**: 20-epoch training with metrics tracking
4. **Performance Analysis**: Loss/accuracy visualization
5. **Model Summary**: Parameter count and architecture details

## ğŸ“Š Model Performance Analysis

### Strengths
- âœ… Deep architecture for feature extraction
- âœ… Batch normalization for training stability
- âœ… Adaptive pooling for parameter efficiency
- âœ… Comprehensive metrics tracking

### Areas for Improvement
- ğŸ”„ Parameter count is relatively high (420K+ parameters)
- ğŸ”„ Could explore more efficient architectures
- ğŸ”„ Learning rate scheduling could improve convergence
- ğŸ”„ Data augmentation could enhance generalization

## ğŸ”® Next Iterations

Future versions will focus on:
1. **Parameter Reduction**: Target <20K parameters
2. **Architecture Optimization**: More efficient designs
3. **Advanced Techniques**: Learning rate scheduling, dropout, data augmentation
4. **Performance Targets**: Achieving 99%+ accuracy

## ğŸ“ Notes

- This is Version 1 of our MNIST model development
- Model achieves good performance but has room for optimization
- Serves as baseline for future improvements
- All training metrics and visualizations are preserved in the notebook

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Project Status**: âœ… Completed - Version 1  
**Next Version**: Parameter optimization and architecture improvements  
**Last Updated**: September 2025
